"""
Drishti IAS Scraper API Endpoints
Chrome-free content scraping with HTTP + Gemini LLM integration

Features:
- Daily current affairs scraping from premium Drishti content using AI
- Editorial content scraping with Gemini 2.5 Flash analysis
- Cloud-compatible approach (no Chrome dependency)
- Content preference logic (Drishti > RSS for duplicate topics)
- Real-time scraping status and performance metrics
- Background task processing for large scraping jobs

Compatible with: FastAPI 0.116.1, Python 3.13.5, Cloud deployment ready
Created: 2025-08-29, Updated: 2025-08-30 (Chrome-free migration)
"""

from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks, Request
from typing import Dict, Any, List, Optional
import asyncio
import logging
from datetime import datetime
from pydantic import BaseModel

# Local imports
from ..core.security import require_authentication, require_admin_access
from ..core.database import get_database, SupabaseConnection
from ..services.drishti_scraper import DrishtiScraper, DrishtiArticle

# Initialize router and logger
router = APIRouter(prefix="/api/drishti", tags=["Enhanced Drishti IAS Scraping"])
logger = logging.getLogger(__name__)

# Global scraper instance for performance optimization
_scraper_instance: Optional[DrishtiScraper] = None

def get_drishti_scraper() -> DrishtiScraper:
    """Get or create Drishti scraper singleton"""
    global _scraper_instance
    if _scraper_instance is None:
        _scraper_instance = DrishtiScraper()
    return _scraper_instance

@router.post("/scrape/daily-current-affairs", response_model=Dict[str, Any])
async def scrape_daily_current_affairs(
    background_tasks: BackgroundTasks,
    user: dict = Depends(require_authentication),
    scraper: DrishtiScraper = Depends(get_drishti_scraper),
    db: SupabaseConnection = Depends(get_database),
    max_articles: int = 20,
    target_date: Optional[str] = None  # Format: "DD-MM-YYYY" or None for today
):
    """
    üîÑ Scrape daily current affairs from Drishti IAS
    
    Features:
    - Chrome-free content extraction with HTTP + Gemini LLM
    - AI-powered UPSC relevance analysis using Gemini 2.5 Flash
    - Smart duplicate detection
    - Cloud-compatible approach for reliable deployment
    - Bulk database operations for performance
    
    Returns comprehensive scraping results with performance metrics
    """
    try:
        # Parse target date if provided
        parsed_date = None
        if target_date:
            try:
                parsed_date = datetime.strptime(target_date, "%d-%m-%Y")
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail="Invalid date format. Use DD-MM-YYYY format (e.g., 30-08-2025)"
                )
        
        date_info = f" for {target_date}" if target_date else " for today"
        logger.info(f"Starting daily current affairs scraping{date_info} requested by user: {user.get('user', 'unknown')}")
        
        # Scrape articles with date-specific targeting
        articles = await scraper.scrape_daily_current_affairs(max_articles=max_articles, target_date=parsed_date)
        
        if not articles:
            return {
                "success": False,
                "message": "No articles scraped from Drishti IAS",
                "data": {
                    "articles_scraped": 0,
                    "articles_saved": 0,
                    "scraping_stats": scraper.get_scraping_stats()
                },
                "timestamp": datetime.utcnow().isoformat()
            }
        
        # Save to database
        saved_count = 0
        duplicate_count = 0
        error_count = 0
        
        for i, article in enumerate(articles, 1):
            try:
                # Debug logging for article validation
                logger.info(f"üîç Processing article {i}: {article.title[:50]}...")
                logger.info(f"   Content length: {len(article.content)} chars")
                logger.info(f"   UPSC relevance: {article.upsc_relevance}")
                logger.info(f"   Content hash: {article.content_hash}")
                
                # Validate article has minimum required content
                if not article.title or len(article.title.strip()) < 10:
                    logger.warning(f"‚ùå Skipping article {i}: Title too short: '{article.title}'")
                    error_count += 1
                    continue
                
                if not article.content or len(article.content.strip()) < 50:
                    logger.warning(f"‚ùå Skipping article {i}: Content too short: {len(article.content)} chars")
                    error_count += 1
                    continue
                
                # Convert to dict for database insert
                article_data = {
                    "title": article.title,
                    "content": article.content,
                    "url": article.url,
                    "published_date": article.published_date.isoformat(),
                    "source": article.source,
                    "category": article.category,
                    "upsc_relevance": article.upsc_relevance,
                    "gs_paper": article.gs_paper,
                    "tags": article.tags,
                    "summary": article.summary,
                    "key_points": article.key_points,
                    "content_hash": article.content_hash,
                    "article_type": article.article_type
                }
                
                logger.info(f"üöÄ Attempting to insert article {i} into database...")
                
                # Try to insert (will handle duplicates via unique constraints)
                result = await db.insert_current_affair(article_data)
                
                if result:
                    saved_count += 1
                    logger.info(f"‚úÖ Successfully saved article {i}: {article.title[:50]}...")
                else:
                    duplicate_count += 1
                    logger.info(f"üîÑ Article {i} was duplicate: {article.title[:50]}...")
                    
            except Exception as e:
                logger.error(f"‚ùå Error saving article {i} '{article.title[:30]}...': {e}")
                error_count += 1
        
        # Get scraping statistics
        scraping_stats = scraper.get_scraping_stats()
        
        logger.info(f"‚úÖ Daily current affairs scraping completed: {saved_count} saved, {duplicate_count} duplicates")
        
        return {
            "success": True,
            "message": f"Successfully scraped {len(articles)} articles from Drishti IAS daily current affairs",
            "data": {
                "articles_scraped": len(articles),
                "articles_saved": saved_count,
                "articles_duplicate": duplicate_count,
                "articles_error": error_count,
                "scraping_performance": scraping_stats,
                "content_type": "daily_current_affairs",
                "source_priority": "premium"  # Drishti content has priority over RSS
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error in daily current affairs scraping: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Daily current affairs scraping failed: {str(e)}"
        )



@router.get("/scraper/status", response_model=Dict[str, Any])
async def get_scraper_status(
    user: dict = Depends(require_authentication),
    scraper: DrishtiScraper = Depends(get_drishti_scraper)
):
    """
    üìä Get Drishti scraper status and performance metrics
    
    Returns:
    - Current scraper health and status
    - Performance statistics and metrics
    - Target URLs and configuration
    - Gemini LLM integration status
    - Cloud deployment compatibility
    """
    try:
        scraping_stats = await scraper.get_scraping_stats()
        
        return {
            "success": True,
            "message": "Drishti scraper status retrieved",
            "scraper_status": {
                "extraction_method": "Chrome-free HTTP + Gemini LLM",
                "cloud_compatible": True,
                "browser_dependency": "None (eliminated)",
                "target_urls": scraper.target_urls,
                "performance_stats": scraping_stats,
                "optimization_features": {
                    "http_requests": "‚úÖ Fast HTTP-only content fetching",
                    "gemini_llm_parsing": "‚úÖ Gemini 2.5 Flash intelligent extraction",
                    "structured_output": "‚úÖ JSON schema with responseSchema",
                    "cloud_deployment": "‚úÖ Railway, Heroku, AWS compatible",
                    "ai_content_analysis": "‚úÖ Semantic content understanding",
                    "batch_processing": "‚úÖ Efficient API usage",
                    "duplicate_detection": "‚úÖ Content hash based",
                    "error_recovery": "‚úÖ Robust retry mechanisms"
                },
                "deployment_benefits": {
                    "no_chrome_dependency": "‚úÖ Perfect for cloud hosting",
                    "resource_efficiency": "‚úÖ Lower CPU/memory usage",
                    "reliability": "‚úÖ No browser timeout issues",
                    "performance": "‚úÖ 20-30s average processing time",
                    "accuracy": "‚úÖ AI-powered content understanding"
                }
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting scraper status: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get scraper status: {str(e)}"
        )



