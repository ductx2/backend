# LiteLLM Router Configuration for UPSC Backend
# Using Vercel AI Gateway with GPT-OSS-120B (Cerebras)
# Provider: Vercel AI Gateway (https://ai-gateway.vercel.sh/v1)
# Model: GPT-OSS-120B ($0.25/M input, $0.69/M output)
#
# Benefits:
# - 10x faster than DeepSeek (3,000 tok/s via Cerebras)
# - Full structured output support (JSON Schema)
# - Built-in fallbacks and load balancing
# - No markup on tokens
# - Spend monitoring included
#
# Updated: 2026-01-31

model_list:
  # GPT-OSS-120B via Vercel AI Gateway (Cerebras backend)
  # OpenAI-compatible API format
  - model_name: gpt-oss-120b
    litellm_params:
      model: openai/gpt-oss-120b
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      base_url: 'https://ai-gateway.vercel.sh/v1'
      rpm: 100 # Rate limit: 100 requests per minute
      timeout: 120
      supports_function_calling: true
      supports_vision: false

router_settings:
  routing_strategy: simple-shuffle # Single model, no routing needed
  num_retries: 3
  timeout: 60
  allowed_fails: 2
  cooldown_time: 15
  enable_pre_call_checks: true

general_settings:
  completion_model: gpt-oss-120b
  disable_spend_logs: false

litellm_settings:
  drop_params: true # Drop unsupported params
  set_verbose: false # Reduce logging noise
  json_logs: true
  num_retries: 3
  request_timeout: 60

health_check_interval: 60
health_check_timeout: 10
